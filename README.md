# UNIFOR - Projeto final - Data Pipeline Logistics

Projeto de demonstraÃ§Ã£o de uma **pipeline de dados orientada a eventos** para simulaÃ§Ã£o de pedidos de vendas. Utiliza **Apache Kafka** para mensageria, **MongoDB** como base de dados NoSQL, e um agente LLM (Agno) para **categorizaÃ§Ã£o e roteamento inteligente dos pedidos**. Um painel interativo com **Streamlit** permite a visualizaÃ§Ã£o das rotas definidas para entrega.

## ğŸ”§ Tecnologias Utilizadas

- Docker & Docker Compose
- Apache Kafka + Zookeeper
- MongoDB
- Python 3.11
- Streamlit
- Agno (para processamento com LLM)

## ğŸ—ºï¸ Arquitetura da SoluÃ§Ã£o



- O **Producer** gera pedidos simulados e envia para o Kafka.
- O **Consumer** consome esses pedidos, faz parsing e grava no MongoDB.
- O **Agente LLM** classifica os pedidos com base em regras de negÃ³cio e os agrupa por rota.
- O **Streamlit Dashboard** exibe os pedidos agrupados por rota.



![alt text](Diagrama_projeto.png)

Toda a comunicaÃ§Ã£o entre os serviÃ§os acontece dentro de uma **rede Docker compartilhada**.

## ğŸ“ Estrutura do Projeto

```plaintext
data-pipeline-logistics/
â”œâ”€â”€ compose.yml                     # OrquestraÃ§Ã£o principal
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml         # ConfiguraÃ§Ã£o alternativa/modular
â”œâ”€â”€ consumers/                     # Consumo do Kafka e persistÃªncia no MongoDB
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ order_consumer.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ producers/                     # GeraÃ§Ã£o contÃ­nua de pedidos simulados
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ order_producer.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ pagseguro/
â”‚       â”œâ”€â”€ gen_order.py
â”‚       â””â”€â”€ dataset/
â”‚           â”œâ”€â”€ customers_llm.json
â”‚           â””â”€â”€ produtos_tech.py
â”œâ”€â”€ llm/                           # Agente LLM de categorizaÃ§Ã£o de pedidos
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ agente_categorizacao.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ categorize.txt
â”œâ”€â”€ streamlit_app.py               # Painel interativo de visualizaÃ§Ã£o
â”œâ”€â”€ requirements.txt               # Requisitos gerais (opcional)
â”œâ”€â”€ Makefile                       # Atalhos Ãºteis para build e execuÃ§Ã£o
â”œâ”€â”€ logo.png                       # Logotipo do projeto
â””â”€â”€ README.md                      # Este arquivo
```

## âš™ï¸ Requisitos

- [Docker](https://www.docker.com/) e [Docker Compose](https://docs.docker.com/compose/)
- [Python 3.11+](https://www.python.org/) (caso deseje executar os scripts localmente)

## ğŸš€ Como Executar

1. **Suba os serviÃ§os principais com Docker Compose:**

```bash
docker compose up --build
```

Isso iniciarÃ¡ os seguintes containers:
- Kafka + Zookeeper
- MongoDB
- Producer (geraÃ§Ã£o de pedidos)
- Consumer (gravaÃ§Ã£o no MongoDB)

2. **Abra o painel de visualizaÃ§Ã£o com Streamlit:**

Em outro terminal:

```bash
streamlit run streamlit_app.py
```

3. **Execute o agente de categorizaÃ§Ã£o de pedidos (opcional):**

Esse agente acessa o MongoDB, agrupa os pedidos por **rota/estado**, e grava a saÃ­da categorizada:

```bash
python llm/agente_categorizacao.py
```



## Contribuidores
* [Felipe Soares](https://github.com/felipesoaresdev/)
* [Kandarpa Galas](https://github.com/kandarpagalas/) 
* [Winiston Freitas](https://github.com/winistonvf)

## ğŸ“ LicenÃ§a

Este projeto Ã© distribuÃ­do com o propÃ³sito de aprendizado e demonstraÃ§Ã£o de arquitetura de pipelines de dados.  
**Uso em produÃ§Ã£o nÃ£o Ã© recomendado sem adaptaÃ§Ãµes.**